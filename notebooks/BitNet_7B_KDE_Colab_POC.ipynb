# BitNet-7B-KDE POC Colab Notebook
# Run each cell in order

# %% [markdown]
# # BitNet-7B-KDE POC on Google Colab
# This notebook runs a proof-of-concept for the BitNet-7B-KDE project using AIMLAPI and Google Drive storage.

# %% [markdown]
# ## 1. Setup Environment

# %%
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# %%
# Clone the repository
!git clone https://github.com/your-username/bitnet-7b-kde.git
%cd bitnet-7b-kde

# %%
# Install dependencies
!pip install -q torch transformers accelerate datasets pyarrow tqdm jsonschema requests python-dotenv
!pip install -q openai anthropic groq google-generativeai
!pip install -q boto3 dropbox supabase google-cloud-storage

# %% [markdown]
# ## 2. Configure Environment for AIMLAPI

# %%
# Create .env file with AIMLAPI configuration
import os
from pathlib import Path

# Get AIMLAPI key from user
from google.colab import userdata
try:
    AIMLAPI_KEY = userdata.get('AIMLAPI_KEY')
    print("✅ Found AIMLAPI_KEY in Colab secrets")
except:
    # Fallback to manual input
    import getpass
    AIMLAPI_KEY = getpass.getpass('Enter your AIMLAPI key: ')

# Create complete .env file
env_content = f"""
############################
# BitNet-7B-KDE POC Config #
############################

# === API PROVIDERS ===
# Using AIMLAPI with OpenAI-compatible interface
OPENAI_API_KEY={AIMLAPI_KEY}
OPENAI_BASE_URL=https://api.aimlapi.com/v1

# Teacher model settings (via AIMLAPI)
TEACHER_PROVIDER=openai
TEACHER_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
TEACHER_API_KEY={AIMLAPI_KEY}
TEACHER_BASELINE_TEMPERATURE=0.7
TEACHER_BASELINE_TOP_P=0.9
TEACHER_BASELINE_MAX_TOKENS=256
TEACHER_TOP_LOGPROBS=20

# === STORAGE (Google Drive) ===
STORAGE_BACKEND=local
LOCAL_STORAGE_PATH=/content/drive/MyDrive/bitnet-7b-kde-poc
CHECKPOINT_DIR=/content/drive/MyDrive/bitnet-7b-kde-poc/checkpoints
DATA_DIR=/content/drive/MyDrive/bitnet-7b-kde-poc/data
LOG_DIR=/content/drive/MyDrive/bitnet-7b-kde-poc/logs

# === MODEL CONFIG (Small for POC) ===
TOKENIZER_NAME=meta-llama/Llama-3.2-1B
MINI_DIM=768
MINI_LAYERS=12
MINI_HEADS=12
MINI_HEAD_DIM=64

# === TRAINING (Small scale for POC) ===
TOTAL_STEPS=500
TRAIN_BATCH_SIZE=2
GRAD_ACCUM_STEPS=4
MAX_SEQ_LEN=256
MAX_TOPK=20
NUM_WORKERS=2

# Optimizer
LR=6e-4
ADAM_BETA1=0.9
ADAM_BETA2=0.95
ADAM_EPS=1e-8
WEIGHT_DECAY=0.1
GRAD_CLIP_NORM=1.0

# Scheduler
SCHEDULER_TMAX=500
SCHEDULER_ETA_MIN=6e-5

# Logging
LOG_INTERVAL=25
CHECKPOINT_INTERVAL=100

# === KD SETTINGS ===
KD_TAU=1.3
KD_CE_WEIGHT=0.25
FORMAT_LOSS_WEIGHT=0.2
KD_TEACHER_TEMPERATURE=0.8
KD_TEACHER_TOP_P=0.95
KD_MAX_TOKENS_PER_PROMPT=256

# === BIT FLIPPING ===
BUDGET_TOKENS=500000
FLIP_FRACTION=0.9

# === DEVICE SETTINGS ===
DEVICE_TYPE=auto
FORCE_CPU=0
USE_AMP=1
TORCH_DTYPE=bf16

# === MISC ===
SEED=42
DETERMINISTIC=0
EVAL_MAX_NEW_TOKENS=256
EVAL_MAX_PROMPTS=10
SAVE_SAMPLE_GENERATIONS=1
"""

# Write .env file
with open('.env', 'w') as f:
    f.write(env_content)

print("✅ Created .env file with AIMLAPI configuration")

# %% [markdown]
# ## 3. Generate Test Prompts

# %%
# Create the generate_prompts.py script if it doesn't exist
generate_prompts_code = '''
import json
from pathlib import Path

def generate_poc_prompts():
    """Generate a small diverse set of prompts for POC testing."""
    prompts = [
        # Technical
        {"prompt": "Explain neural networks in simple terms"},
        {"prompt": "What is gradient descent?"},
        {"prompt": "How does attention mechanism work?"},
        
        # Code
        {"prompt": "Write a Python function for bubble sort"},
        {"prompt": "Create a simple class for a bank account"},
        
        # Math
        {"prompt": "Calculate the area of a circle with radius 5"},
        {"prompt": "What is 25% of 180?"},
        
        # Creative
        {"prompt": "Write a haiku about machine learning"},
        {"prompt": "Describe a robot's first day at school"},
        
        # Factual
        {"prompt": "What is the speed of light?"},
        {"prompt": "Who developed the theory of relativity?"},
        
        # Instructions
        {"prompt": "How to make coffee step by step"},
        {"prompt": "Explain how to train a neural network"},
        
        # JSON/Structured
        {"prompt": "Create a JSON object for a user profile"},
        {"prompt": "Generate a markdown table of planets"},
    ]
    return prompts

# Generate and save prompts
prompts = generate_poc_prompts()
Path("data").mkdir(exist_ok=True)
with open("data/prompts.json", "w") as f:
    json.dump(prompts, f, indent=2)
    
print(f"✅ Generated {len(prompts)} POC prompts")
'''

with open('generate_prompts_temp.py', 'w') as f:
    f.write(generate_prompts_code)

!python generate_prompts_temp.py

# %%
# Display generated prompts
import json
with open('data/prompts.json', 'r') as f:
    prompts = json.load(f)
    print(f"📝 Generated {len(prompts)} prompts for testing:")
    for i, p in enumerate(prompts[:5]):
        print(f"  {i+1}. {p['prompt']}")
    if len(prompts) > 5:
        print(f"  ... and {len(prompts)-5} more")

# %% [markdown]
# ## 4. Test Setup

# %%
# Run sanity check
!python -m scripts.sanity_check

# %% [markdown]
# ## 5. Run Data Collection Pipeline

# %%
# Step 1: Collect teacher baseline samples
print("📚 Collecting teacher baseline samples...")
!python -m scripts.run_teacher_baseline

# %%
# Check teacher outputs
!ls -la data/teacher/
!echo "Sample teacher output:"
!head -n 2 data/teacher/baseline_*.jsonl 2>/dev/null || echo "No baseline files yet"

# %%
# Step 2: Collect KD traces with logprobs
print("🎯 Collecting KD traces...")
!python -m scripts.collect_kd_traces

# %%
# Check KD outputs
!ls -la data/kd/
!echo "KD shard files created"

# %% [markdown]
# ## 6. Train Mini BitNet Model

# %%
# Start training (reduced steps for POC)
print("🚀 Starting BitNet training (POC - 500 steps)...")
!python -m scripts.train_mini_bitnet

# %%
# Check training outputs
!ls -la /content/drive/MyDrive/bitnet-7b-kde-poc/checkpoints/
!echo "Latest checkpoint:"
!ls -t /content/drive/MyDrive/bitnet-7b-kde-poc/checkpoints/*.pt | head -1

# %% [markdown]
# ## 7. Evaluate Model

# %%
# Run evaluation
print("📊 Evaluating model and computing QEI...")
!python -m scripts.eval_and_qei

# %%
# Display evaluation results
!echo "Evaluation results:"
!cat outputs/eval_*.json 2>/dev/null || echo "No evaluation results yet"

# %% [markdown]
# ## 8. Test 7B Dry Run (Memory Estimation)

# %%
# Run 7B memory estimation
print("💾 Testing 7B model memory requirements...")
!python -m scripts.dry_run_7b_memory

# %% [markdown]
# ## 9. Results Summary

# %%
# Generate summary report
import json
import glob
from pathlib import Path

print("=" * 50)
print("BitNet-7B-KDE POC Results Summary")
print("=" * 50)

# Check data collection
teacher_files = glob.glob("data/teacher/*.jsonl")
kd_files = glob.glob("data/kd/*.parquet")
print(f"\n📚 Data Collection:")
print(f"  - Teacher samples: {len(teacher_files)} files")
print(f"  - KD traces: {len(kd_files)} shards")

# Check training
ckpt_files = glob.glob("/content/drive/MyDrive/bitnet-7b-kde-poc/checkpoints/*.pt")
if ckpt_files:
    latest_ckpt = max(ckpt_files, key=os.path.getctime)
    print(f"\n🧠 Training:")
    print(f"  - Checkpoints saved: {len(ckpt_files)}")
    print(f"  - Latest: {Path(latest_ckpt).name}")
    
    # Load health file if exists
    health_file = latest_ckpt.replace('.pt', '_health.json')
    if os.path.exists(health_file):
        with open(health_file, 'r') as f:
            health = json.load(f)
            print(f"  - Steps completed: {health.get('step', 'N/A')}")
            print(f"  - Tokens seen: {health.get('seen_tokens', 0):,}")
            print(f"  - Activation bits: A{health.get('activation_bits', 8)}")
            print(f"  - Last loss: {health.get('last_loss', 'N/A'):.4f}")

# Check evaluation
eval_files = glob.glob("outputs/eval_*.json")
if eval_files:
    latest_eval = max(eval_files, key=os.path.getctime)
    with open(latest_eval, 'r') as f:
        eval_data = json.load(f)
    print(f"\n📊 Evaluation:")
    print(f"  - QEI Score: {eval_data.get('qei', 'N/A'):.3f}")
    print(f"  - Perplexity: {eval_data.get('perplexity', 'N/A'):.2f}")

print("\n✅ POC Complete!")

# %% [markdown]
# ## 10. Next Steps for 7B Model
# 
# Based on this POC, to scale to 7B:
# 
# 1. **Increase model dimensions:**
#    - MINI_DIM: 768 → 4096
#    - MINI_LAYERS: 12 → 32
#    - MINI_HEADS: 12 → 32
# 
# 2. **Expand training:**
#    - TOTAL_STEPS: 500 → 10000+
#    - More diverse prompts (1000+)
#    - Larger batch sizes with gradient accumulation
# 
# 3. **Use better hardware:**
#    - Upgrade to Colab Pro+ or A100 GPU
#    - Or use cloud providers (Lambda Labs, RunPod)
# 
# 4. **Optimize memory:**
#    - Enable gradient checkpointing
#    - Use DeepSpeed or FSDP for model parallelism
#    - Consider 8-bit Adam optimizer

# %%
# Save notebook state to Drive
!cp *.ipynb /content/drive/MyDrive/bitnet-7b-kde-poc/
print("💾 Notebook saved to Google Drive")
