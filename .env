############################
# RUNTIME / COLAB / DRIVE  #
############################

# Mount Drive automatically when running in Colab (1=yes, 0=no)
AUTO_MOUNT_GDRIVE=1
# Where Colab mounts Drive (rarely needs changes)
GDRIVE_MOUNT_POINT=/content/drive
# REQUIRED: Root project folder on Drive. All artifacts go here.
# Example: /content/drive/MyDrive/bitnet_poc
DRIVE_ROOT=/content/drive/MyDrive/bitnet_poc

# Subfolders (override if you want custom names/locations)
CHECKPOINTS_DIR=${DRIVE_ROOT}/checkpoints
DATA_DIR=${DRIVE_ROOT}/data
REPORTS_DIR=${DRIVE_ROOT}/reports
LOGS_DIR=${DRIVE_ROOT}/logs

# Persist caches to Drive so downloads survive runtime resets
HF_HOME=${DRIVE_ROOT}/.hf
TRANSFORMERS_CACHE=${HF_HOME}/transformers
HF_DATASETS_CACHE=${HF_HOME}/datasets
TORCH_HOME=${DRIVE_ROOT}/.torch

# Reduce tokenizer thread spam; recommended
TOKENIZERS_PARALLELISM=false
# Better CUDA allocator defaults; tweak if OOMs
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128


############################
# PROVIDER & API SELECTION #
############################

# One of: openai | anthropic | groq | aimlapi | gemini
PROVIDER=openai

# Default model per provider (override per script/Makefile if needed)
OPENAI_MODEL=gpt-4o-mini
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
GROQ_MODEL=llama-3.1-70b-versatile
AIMLAPI_MODEL=gpt-4o-mini
GEMINI_MODEL=gemini-1.5-pro

# API KEYS (fill what you use)
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GROQ_API_KEY=
AIMLAPI_API_KEY=
GEMINI_API_KEY=

# Optional base URLs for OpenAI-compatible providers
OPENAI_BASE_URL=
GROQ_BASE_URL=https://api.groq.com/openai/v1
AIMLAPI_BASE_URL=https://api.aimlapi.com/v1


############################
# TRAINING / RUNTIME OPTS  #
############################

# bf16 | fp16 | fp32
TORCH_DTYPE=bf16
# Try FlashAttention on A100/H100 if available
ENABLE_FLASH_ATTN=1
# Disable Weights & Biases if you don't use it
WANDB_DISABLED=1
WANDB_PROJECT=bitnet-poc
WANDB_ENTITY=
CUDA_VISIBLE_DEVICES=

# Tokenizer / prompt template (override per experiment if needed)
TOKENIZER_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
TEMPLATE=<|user|>\n{prompt}\n\n<|assistant|>\n

# (Optional) Hugging Face token if you push/pull private repos
HF_TOKEN=
