############################
# RUNTIME / COLAB / DRIVE  #
############################
AUTO_MOUNT_GDRIVE=1
GDRIVE_MOUNT_POINT=/content/drive
DRIVE_ROOT=/content/drive/MyDrive/bitnet_poc
AUTO_CREATE_DIRS=1

# Subfolders
CHECKPOINTS_DIR=${DRIVE_ROOT}/checkpoints
DATA_DIR=${DRIVE_ROOT}/data
REPORTS_DIR=${DRIVE_ROOT}/reports
LOGS_DIR=${DRIVE_ROOT}/logs

# Persist caches on Drive
HF_HOME=${DRIVE_ROOT}/.hf
TRANSFORMERS_CACHE=${HF_HOME}/transformers
HF_DATASETS_CACHE=${HF_HOME}/datasets
TORCH_HOME=${DRIVE_ROOT}/.torch

TOKENIZERS_PARALLELISM=false
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
LOG_LEVEL=INFO
SEED=1234
DETERMINISTIC=0

################################
# ARTIFACT STORAGE BACKEND     #
################################
# one of: local | gdrive | onedrive | dropbox | icloud | s3 | box | nextcloud | webdav | firebase | amplify | supabase | mongodb_atlas | dynamodb
STORAGE_BACKEND=gdrive

# --- Google Drive (used when STORAGE_BACKEND=gdrive) ---
GDRIVE_PROJECT_ROOT=${DRIVE_ROOT}

# --- OneDrive ---
ONEDRIVE_ROOT=/BitNet-7B-KDE
ONEDRIVE_TENANT_ID=
ONEDRIVE_CLIENT_ID=
ONEDRIVE_CLIENT_SECRET=
ONEDRIVE_REFRESH_TOKEN=

# --- Dropbox ---
DROPBOX_ACCESS_TOKEN=
DROPBOX_ROOT=/bitnet_poc

# --- Apple iCloud Drive (WebDAV not officially supported; placeholder) ---
ICLOUD_DRIVE_DIR=
ICLOUD_SESSION_TOKEN=

# --- Amazon S3 (or S3-compatible) ---
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=
S3_BUCKET=
S3_PREFIX=bitnet_poc/
S3_ENDPOINT_URL=                 # optional for MinIO/Wasabi/etc.

# --- Box ---
BOX_DEVELOPER_TOKEN=
BOX_ROOT_FOLDER_ID=0

# --- Nextcloud (WebDAV) ---
NC_WEBDAV_URL=                   # e.g. https://cloud.example.com/remote.php/dav/files/<user>/
NC_USERNAME=
NC_APP_PASSWORD=
NC_ROOT_PATH=/bitnet_poc

# --- Generic WebDAV (Custom tool / URL prompt case) ---
WEBDAV_URL=
WEBDAV_USERNAME=
WEBDAV_PASSWORD=
WEBDAV_ROOT_PATH=/bitnet_poc

# --- Firebase Storage ---
FIREBASE_PROJECT_ID=
FIREBASE_CREDENTIALS_JSON=       # path to service account json (mounted/uploaded)
FIREBASE_STORAGE_BUCKET=

# --- AWS Amplify (metadata only; storage still via S3 usually) ---
AMPLIFY_APP_ID=
AMPLIFY_ENV=
AMPLIFY_REGION=

# --- Supabase ---
SUPABASE_URL=
SUPABASE_ANON_KEY=
SUPABASE_SERVICE_ROLE_KEY=
SUPABASE_BUCKET=bitnet-poc
SUPABASE_PREFIX=artifacts/

# --- MongoDB Atlas (for metadata/indexing, not blobs) ---
MONGODB_ATLAS_URI=
MONGODB_DB_NAME=bitnet_poc
MONGODB_COLLECTION_PREFIX=artifacts_

# --- Amazon DynamoDB (for metadata/indexing) ---
DYNAMO_REGION=
DYNAMO_TABLE_PREFIX=bitnet_poc_

############################
# PROVIDER & API SELECTION #
############################
# one of: openai | anthropic | groq | aimlapi | gemini
PROVIDER=openai

# default model per provider
OPENAI_MODEL=gpt-4o-mini
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
GROQ_MODEL=llama-3.1-70b-versatile
AIMLAPI_MODEL=gpt-4o-mini
GEMINI_MODEL=gemini-1.5-pro

# API keys
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GROQ_API_KEY=
AIMLAPI_API_KEY=
GEMINI_API_KEY=

# optional OpenAI-compatible base URLs
OPENAI_BASE_URL=
GROQ_BASE_URL=https://api.groq.com/openai/v1
AIMLAPI_BASE_URL=https://api.aimlapi.com/v1

############################
# TEACHER: BASELINE & KD   #
############################
# keep separate from PROVIDER so KD can use another vendor that supports top_logprobs
TEACHER_PROVIDER=openai          # openai | anthropic | groq | aimlapi | gemini
TEACHER_MODEL=gpt-4o-mini
TEACHER_BASE_URL=
TEACHER_API_KEY=                 # if empty, fallback to provider key above

TEACHER_CTX_LEN=8192
TEACHER_TOP_LOGPROBS=20          # KD needs this; not all vendors support top-k logprobs

# deterministic baseline
TEACHER_BASELINE_TEMPERATURE=0.0
TEACHER_BASELINE_TOP_P=1.0
TEACHER_BASELINE_MAX_TOKENS=256

# KD sampling
KD_TEACHER_TEMPERATURE=0.8
KD_TEACHER_TOP_P=0.95
KD_MAX_TOKENS_PER_PROMPT=512

############################
# TRAINING / RUNTIME OPTS  #
############################
TORCH_DTYPE=bf16                 # bf16 | fp16 | fp32
ENABLE_FLASH_ATTN=1
USE_AMP=1
CUDA_VISIBLE_DEVICES=

# tokenizer / prompt template
TOKENIZER_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
TEMPLATE="<|user|>\n{prompt}\n\n<|assistant|>\n"

# Hugging Face (optional)
HF_TOKEN=

# data shaping
MAX_SEQ_LEN=256
MAX_TOPK=20
TRAIN_BATCH_SIZE=4
NUM_WORKERS=0

# optimizer / scheduler
LR=6e-4
ADAM_BETA1=0.9
ADAM_BETA2=0.95
ADAM_EPS=1e-8
WEIGHT_DECAY=0.1
GRAD_CLIP_NORM=1.0
GRAD_ACCUM_STEPS=1
SCHEDULER_TMAX=1000
SCHEDULER_ETA_MIN=6e-5

# loop control
TOTAL_STEPS=1000
LOG_INTERVAL=50
CHECKPOINT_INTERVAL=200

# A8â†’A4 flip based on true seen tokens
BUDGET_TOKENS=1000000
FLIP_FRACTION=0.9

# KD + auxiliary losses
KD_TAU=1.3
KD_CE_WEIGHT=0.25
FORMAT_LOSS_WEIGHT=0.2

############################
# EVAL / QEI               #
############################
EVAL_MAX_NEW_TOKENS=256
EVAL_MAX_PROMPTS=10
SAVE_SAMPLE_GENERATIONS=1

############################
# WANDB (optional)         #
############################
WANDB_DISABLED=1
WANDB_PROJECT=bitnet-poc
WANDB_ENTITY=

# Training parameters (missing from current .env)
MINI_DIM=768
MINI_LAYERS=12
MINI_HEADS=12
MINI_HEAD_DIM=64

# Teacher settings
TEACHER_PROVIDER=openai
TEACHER_MODEL=gpt-4o-mini
TEACHER_API_KEY=${OPENAI_API_KEY}  # or set separately
TEACHER_BASELINE_TEMPERATURE=0.0
TEACHER_BASELINE_TOP_P=1.0
TEACHER_BASELINE_MAX_TOKENS=256
TEACHER_TOP_LOGPROBS=20
KD_TEACHER_TEMPERATURE=0.8
KD_TEACHER_TOP_P=0.95
KD_MAX_TOKENS_PER_PROMPT=512

# Training hyperparameters
LR=6e-4
ADAM_BETA1=0.9
ADAM_BETA2=0.95
ADAM_EPS=1e-8
WEIGHT_DECAY=0.1
GRAD_CLIP_NORM=1.0
GRAD_ACCUM_STEPS=1
SCHEDULER_TMAX=1000
SCHEDULER_ETA_MIN=6e-5
TOTAL_STEPS=1000
LOG_INTERVAL=50
CHECKPOINT_INTERVAL=200
BUDGET_TOKENS=1000000
FLIP_FRACTION=0.9
KD_TAU=1.3
KD_CE_WEIGHT=0.25
FORMAT_LOSS_WEIGHT=0.2
MAX_SEQ_LEN=256
MAX_TOPK=20
TRAIN_BATCH_SIZE=4
NUM_WORKERS=0
EVAL_MAX_NEW_TOKENS=256
EVAL_MAX_PROMPTS=10
SAVE_SAMPLE_GENERATIONS=1
